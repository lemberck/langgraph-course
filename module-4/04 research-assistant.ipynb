{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5c2153",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/research-assistant.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239974-lesson-4-research-assistant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b",
   "metadata": {},
   "source": [
    "# Research Assistant\n",
    "\n",
    "## Review\n",
    "\n",
    "We've covered a few major LangGraph themes:\n",
    "\n",
    "* Memory\n",
    "* Human-in-the-loop\n",
    "* Controllability\n",
    "\n",
    "##### Now, we'll bring these ideas together to tackle one of AI's most popular applications: research automation. \n",
    "\n",
    "Research is often laborious work offloaded to analysts. AI has considerable potential to assist with this.\n",
    "\n",
    "However, **research demands customization: raw LLM outputs are often poorly suited for real-world decision-making workflows.** \n",
    "\n",
    "##### Customized, AI-based [research and report generation](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag) workflows are a promising way to address this.\n",
    "\n",
    "## Goal\n",
    "\n",
    "##### Our goal is to build a lightweight, multi-agent system around chat models that customizes the research process.\n",
    "\n",
    "`Source Selection` \n",
    "* Users can choose any set of input sources for their research.\n",
    "  \n",
    "`Planning` \n",
    "* Users provide a topic, which we'll break it up into a set of sub-topics. Then, the system generates a team of AI analysts, each focusing on one sub-topic.\n",
    "* `Human-in-the-loop` will be used to refine these sub-topics before research begins.\n",
    "  \n",
    "`LLM Utilization`\n",
    "* Each analyst will conduct in-depth interviews with an expert AI using the selected sources.\n",
    "> Orchestration of a dialog between each analyst and an expert that has access to the sources. -> AI-AI roleplay\n",
    "* The interview will be a multi-turn conversation to extract detailed insights as shown in the [STORM](https://arxiv.org/abs/2402.14207) paper.\n",
    "* These interviews will be captured in a using `sub-graphs` with their internal state. \n",
    "   \n",
    "`Research Process`\n",
    "* Experts will gather information to answer analyst questions in `parallel`.\n",
    "* And all interviews will be conducted simultaneously through `map-reduce`.\n",
    "\n",
    "`Output Format` \n",
    "* The gathered insights from each interview will be synthesized into a final report.\n",
    "* We'll use customizable prompts for the report, allowing for a flexible output format. \n",
    "\n",
    "![Screenshot 2024-08-26 at 7.26.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb164d61c93d48e604091_research-assistant1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23991e9-51b3-4e9f-86a0-dec16aa7d1e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba917800-10e4-4e2a-8e9e-30893b731e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe9ff57-0826-4669-b88b-4d0501a509f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419257b-2c6b-4d68-ae38-4a266cc02982",
   "metadata": {},
   "source": [
    "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5102cf2e-0ca9-465b-9499-67abb8132e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe5d93-e353-44bb-be3e-434654bcb7ea",
   "metadata": {},
   "source": [
    "## Generate Analysts: Human-In-The-Loop\n",
    "\n",
    "##### First graph to generate the analysts and review them using human-in-the-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee8e60-e548-49b1-88ec-a4f3aef2174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Analyst(BaseModel): # Analyst model that standardizes all analysts characteristics\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the analyst.\",\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"Name of the analyst.\"\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"Role of the analyst in the context of the topic.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Description of the analyst focus, concerns, and motives.\",\n",
    "    )\n",
    "    # @property lets you access a method like a read-only attribute, so analyst.persona returns a formatted string without needing analyst.persona()\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "class Perspectives(BaseModel): # for structed output. This is the list of analysts\n",
    "    analysts: List[Analyst] = Field(\n",
    "        description=\"Comprehensive list of analysts with their roles and affiliations.\",\n",
    "    )\n",
    "\n",
    "class GenerateAnalystsState(TypedDict): # Analyst Generation Graph State\n",
    "    topic: str # The Overall Research topic\n",
    "    max_analysts: int # Number of analysts to generate (also the sub-topics, as 1 analyst = 1 sub-topic)\n",
    "    human_analyst_feedback: str # Human feedback\n",
    "    analysts: List[Analyst] # Analysts asking questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd088ff5-4c75-412c-85f0-04afd0900bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKkAAAF3CAIAAABR9PyTAAAQAElEQVR4nOydB1wURxvGZ69y9KaAIFUUlYgdS6JRQWOLDaOx9xZLbMRYotHYiBoLQcVeosbeEltsMfaGChakClKl1zuufO/dwnni3Xnnx7nHzv5j+O3tzNZn5p13ys5wZDIZYsASDmLAFUZ7fGG0xxdGe3xhtMcXRnt8MWrtM1+XRt3My0oVCYulSCYrK1PsJRCSIUIOkkplBJuQSeTVVDaHJRFLYSdEkEnlEVlsJJUgFouAYJlUHke+LYfcQGT9loDtilD4K5WWV3o5bCRWHK64CpJJyu+KYMnPrzxKuUHC47MIlszElO3gxmsWaM3j8ZCxQhhh/T41oeifP9Lzs6Rwa2wewTchuDwWwUFSoVwbUnv4X/HSy5VAcqUJKSQCApLFO9pDBHlsxR7EkutNyAj5UTLyPPIIcnXhOIX2b4WEfCFWKl1xBvIG4OoEQe55JwgO4sNtSMpESFgiEZchDgc5uJv0meSCjA/j0r6kULJnebyoGJlZsT773Kp5gB2q5lw5mB4bVViSL7Nz5n07yxUZE0ak/bHfk1/HlDq48/pPM6539P9TlCc6+ntKfpa4RWfrll3skXFgLNpvWxAH9nnsMk9EX+IiC87vTrd35gUZR+I2Cu13LUmwtOMYZ6FY5WxbEOPd2LJdv5qIaqjXPnxurF0tbr/JdLPzWtj6U6y5BWfgbDdEKSxEKTt/jrevxcNKeGDMYq/CPMlfO1IQpVCp/dndKWVCad/JtRF+jPnFMyGy+E1KCaIOKrWPeVgc9H0thCveTcyOrKcy61Om/f6QREs7tk1NAcKVzkOcwNe68VcmogjKtM9KLes6yhHhjXtD06jr+YgiqNH+/O5UvgDVqIVvpif5apiTqFSWmUpNqU+N9q+ii2u4fGrh58yZc+LECaQ/gYGBr1+/RoaBb8a6cSIbUQE12otKZA1bW6JPy9OnT5H+pKam5uTkIINRw4WXnSpEVEBB2052unDfyqTJa+ogw3D9+vXdu3dHRUXZ29v7+flNmTIFNpo3b06GmpubX7lypbCwcO/evTdv3oyNjYXQ9u3bT5w40cTEBCIEBwez2WwnJyc4yfjx4zdv3kweCHFWr16Nqpr7l7LvnsuesNJQb0MLFOT7pOdFbIMNG3j+/Pm0adNatGhx+PBhUDE6OnrRokVIkSDg74IFC0B42Dhw4MDOnTuHDh26du1aiH/hwoXw8HDyDFwuN0bBmjVrgoKCIALshMLCEMIDrnUFUgmiBArGbhTkStiKnnJDEBERAdl31KhRLBbL0dGxQYMGoOL70YYMGdKpUycPDw/y56NHj27cuDF16lTYho75lJSUPXv2kGbA0IDfI5UiSqBi3I5M/oKRYWjcuHFpaen333/v7+/frl272rVrK629KpC5weAvXLgQDINYLIY9tra2ylBIE59G+HIo6lGhwOabWrElBkvqPj4+69evr1GjxoYNG/r06TNp0iTI0+9Hg1Aw8hDh+PHj9+7dGzlypGoon89Hnwp5Bc9QGeEDUKC9oxsfGbKEa9OmDZTrp06dgpI+Ly8PbACZs5WAe3vkyJEBAwaA9lAuwJ6CggJEEWmxpSyKGtgouGwtTzOJFL16aZDXff/+fSi5YQOyfo8ePWbOnAm6Qj1NNU5ZWVlJSUnNmuU96CKR6N9//0UUkfC8mEPRcE5qkhzPhIi6ZhDtwcKDe3/06FGolEdGRoI/D4kAKmxgxkHsW7dugYUHN9Dd3f3kyZPJycm5ubmLFy8GLyE/P7+oqOj9E0JM+AsVATgbMgBvXpfaOnIRFVCjvYMbPznWIA2Z4MCDJV+1ahU0xo0bN87MzAzKdQ5H7tKC83/37l2wBJDply1bBt4cVOF69+7dsmXLyZMnw8+AgADw8Cud0MXFpWfPnps2bQIXARmAolxZ0462iAooG7cTOj1m8m8UNGgYFf8ey4i6WTAxxAtRAWX9eAJz9h8rEhHegPAeDU0RRVD2Xc6g4NrbfkrQEgGMdln5lzjvIJFIoMAmNLQQQJ3N2toaGQBoNYIqg9og8BahwUDtLXl6em7fvl3tUbfOZErEsq+GOyGKoHKs5qG1rwrzJCMXeqgN/bh6l4WFBTIYmm5JKBRqahKABAE9CGqDoNT7opet35fUFPaI8nG6m+fEejcx7zjAAWHG7qUJXB7r29lUDlKleJzu+BVez+8WPLmZhXDi4JrEMqGUWuGRkXybsXF2TJMAq1ZdaiAM2B+SyOIQA2ZQPyzdWL7J2hgcY1OTO3AWxZ8rGJrtC+MJQjZykVF8emZE32LuXBRXmCdt2smqTQ8aGoCT4a+TXpQ4ewt6T3BGxoFxfYN95/yb+xdyYcO5jqDzYAcT82o/LQh0W9w6nfPmtYjLZ/Wb7GTrZETDU41x7gVo7Xp+p0BUKiNYiG9KWNlxoSGIZ8IRi9/eKks+YULFNoHkmxWB8rkXZOX74fmk0vJqt2I3odhPVMy5UR6gmMhDPjGDREZU7Cg/T8Vf+YsiK/DlJyfn44A9UnkAUXF9NosQicTCIklhjri4UH5fZlbslt3s6jf71OMTP4gxaq/k2onM5Ogi+RuUQpMOkpS9vVWlwIrtd55CGaScl6V8f4U8ctnkM64gUrzyNhm59qg8bkVqeF97xYwf8ihsNiGRkLNvKFJUxdm5XBbBkfL4LAsbjmt9s6YdKKu+fxCj1t7QzJ8/v23btl27dkVYgvU8W2KxmOziwxNGe0Z7LGG0xxfoJ4T+N4QrTL5n8j2WMNrjC6M9vjDa4wujPb4w2uMLoz2+MNrjC9O2gy9MvscXRnt8YbTHF6a8xxSpYrwfi0Xx1ykUgq/2mBt8xGiPMIbRHl/wfXjMHT3E5HuEMfg+vEwmq1UL3xVbEM7aQ6ZPSkpCGIO19pXm28QNRnt8YbTHF0Z7fMFae4mEohUrjAN8ezKQ/Bt6Ns5ZH2vtMTf7eDdsMdpjC6M9vjDa4wujPb4w2uMLoz2+MNrjC6M9vmCuPY7zajZu3Jgg51utAF7C559/bqBV0IwWHNt0W7duzXoXBweHESNGIMzAUfvhw4errnoNeHt7N2vWDGEGjtq3atWqUaNGyp9WVlYDBw5E+IFpP96wYcOUWd/Dw6Nt27YIPzDV3s/Pr0mTJrBhZmY2YMAAhCUf4+dfPZZWWoj0GvOiWKLg7UoXusQvX7dAqliWQLf46N01FbTGJ/IL8h9FPOLyuK38/XV6B4Tq0hzylRR0j88mCIk+71nHpyg/ORvZOHBadrZHeqKf9gfXJrxJFrM4iEWwxGV6PYwcqc7il2svXw1FN11YilRSvpTJh9MLueSKYikMQserqCYvRC6XoVt8xe19OP47l6h4HF3g8pFUAv9kzQNtWnS2QzqjR9vO2T0p2anioFmuAgEPMRgZCVF5/x3PNLVkN2yl63LAuub7ExuTMlOFA2bWQQxGzN6lMR362/m0sNElsq6+XkqcsGUX4132h4HEwY13469sHSPrpH3sk3z46+HLaG/sePlZCYt19cN0Ku9FxXJvgsH4EVjyJGW6RtZJe4lUD7eTgUqkelQOse7DxRxGe3zRWXsdGtcYqhe653tG/OqAPg30OmuP8bK51Ql9cihT3uMLoz2+MNrjC6M9rVAU91XapgvdyYhg/PxqgGJAia5K6aS93Mdn/PxqgUyPLKpbHy6j+4c4cvRAQGd/RD16SKXzWE0jM/l9+gWmpL5GtODY8YPLVy5Enxzd23aQ8ZCWlpqbm4PowosXTxEV6Ko9ob+vd/PmtXUbVmZmZtTxqtu79zddv/oadi5cFMxmsx0cnA78ufvnRSHtvuiYnZ0VtnFNZNSj0tLSFi1aDxsypnZtN/IMR4/9eevWtWfPInl8vl+jpqNHf+dcy+VhxL0ZMydA6OAhvdq2bf/L4tVisXjb9rBbt//LyEjz9W3cp9c3rVp9rsvtXbp87vGTh/n5efV9fIcOHdOkcXOkyIV79m5duyZ84c/BCQlxnp51+gcN/qpLTy23pHraadPH8nn8kJWhyj0LfpqVlf0mLHTnq1cJO3Zuinh0XyaTNWzYaOA3wz77rPH3M8Y9evQAop0//9fmTXu969Q7cnT/uXOnk5IT3Vw9mjdvNWrkRHhjSEcIPZxyHW0+IdMz48ObXbBw1uhR361Yvv7zzzuE/Lr4n4tnYT+Xy42Lj4F/S5esafRZE4lEMn3meHgd07+fu33rnzbWtpO+G/46JRliPnkSsSH014YN/RYvXjXnh59zcrKXLpsP+0Gh5UvXwsYfe0+A8LCxfkPI4SP7+vQesO+PU+3bdQLNrv57UfvtQTpbuny+UCiEMy9butbV1X3e/OmQCsk7LCwsgHPOnrng0j9327cLgJtPT0/TckuqdPuq1/0Hd8hTkReCRNk5sLtIJAKZQcWVKzas/nUjh82BK0IoJLL69X07d+5++eK9ut4+R48e2PvH9qB+gw7sO92zZ7+//j4OmQTpjqzq++/1tviQwCFPBwZ0he0WzVsVFRUWFxchhf1IS0vZFLbHxMQEfkZE3IfcsHrVxqZNWsDPiRO+v37j6pEj+6ZOCW7Q4LMd2w66uLiSKxyIy8rmzp+el59nZWmleiHQ79z504O+HfF1z37ws1vXXpGRj3bv2QKJQMvtwdW3hh8QCARWVvJRrZDvT5w8/CQygjyqrKxs+LBxcAOw3aVzD3iWmJgXDg6OutxShw6dQ8NWgUUB/eDnf9evwN+OHbskJSVCWunX91sQGPYs/GnFo8cP3v8CHHbWq9egS5cesN2je58mTVqUFBcjw2CQ8h5sWmzcywCF8CQTxk9TboMpI4UH4HVDPiOFR4qU0divGTw/Ukx6mZKS/HvY6mfPI4uKisgIuTnZlbSPjn4GWapF89bKPXCGM2dPvp9KKgFpceu2UDA5WVlvyk+u4kP4+DQkNywsLOEvWAIdb4nH4wV06vrPP2dI7a9du9S2TXtLC0soCKytbVaELAoM6AZ36OvrRxYxlYD94Vs2gKVp1KhJ69btKhUoVYtu2hP6+fkghlQq5fNN1IZCSanchncKmaxDp3feArwj+Hv9+tX5P80cPGjk+HHTvLy8792/HfzD5PfPRqoyZdroSvtzsrO0aA82fNr0MU2btFwwbxnkZkhzgV1aqUZQ69/oeEs9uvc9fuIQlFx2tva371yHS8BOPp+/7rctYMOheALvpFYtlxHDxgUGdqt0LKQYU1MzMH4rQ34G6/Lll4Hjx061t6+BdEUPv0w37WWETJ9GA8jKLBYL7PwHY9rZ2YPhXfrLb6o72Sy5a3P672PgCo0Z/R25k9RYzRkU72XmjHnOzrVV99es6Yg0c+XqBUigUGbD1dG7OV4LOt4SJAsows+cOeHt7SMQmPr7l3/oCV4FFGojR0x48OAOWKZlK35yc/ckiwAl8N7A1MM/cDMh2s7d4fAal737frRC6J5LdS3vCb0aDVgsKLTAniv3bNkaCu/6u0kzKsX08qpbUlICOimNG9TadQbApgAAEABJREFUra3k+R7cb0cHJ2VMMJ5qr+Xi7MpXGBKlCYViFQodU1NTpBk4ORhzUnjgg76h8ihdbgkp3A7w0ZKTX4H9J50DcGuinj6Gyg6Ud23atIME8VW3tlBgVdIePPy6det7eHi5u3vCv4LCgr/+Pob0QKq7r2eotp1ePYPu3r3558E9UCUDN2r/gV3wPO9Ha9a0ZcuWbVatWgJGOC8vF0zlhIlDz549CUFQM7x77xYcDg7RocN/kPHT0lPhb21Xd/h75cqFp88iQeMRw8eDcwdOOCQvUHFW8KS161Zovz1PT28o5k+eOgInv33nBuQwcPqgiqj9KC23VImOHbpkZWWCwYdEQO6BdAOl+MZNa5NfJ4Hf98e+HXAS34Z+EAQWCyqNDx7ehVR78dLZnxbNvnHjX/BXbt3679p/l8g4hsBQbTvgqeYX5O2Sm6wiMOzjxk5RvoVKQIUNNFj8y49Pnz6Bmj14iH37ymdCGDVqErhj8xfMAMPQt89AsM+pqa/n/Dh13txfAjp9BRVucL/hvfy2ZvPAAcPAfuw7sBMkNDMzb9ig0cyZ87XfXqeOXRIT4yDF/LZ2OVRDfgheBNl03/6dBQX5kO00HaXllirFhBTZrJl/Zka6MsWDEzdj+tyduzYfPLQXfjZv5r9m9SbI2bDds3tfMACzg7+D6t/MGfNDf181b4HcQNra2oHx7x80BBkGnb7Hi7qZf+VQxrCFzMd4ugIWqP+ArpDiu3frjT4hqfEl53a+nrJWJ6V068dDzLcZugLtza9Tko4eO+Dm5qHJ1BkOmT4NsLr135Nd+NUKMOD79+9UGwTedej67cgwQIG9ddvv0Dyw6KeVxCcf9CD/gL9q2/WqY76HBlFoYlMbBO2pyGBA7R/+oeqAQdp2jAELcwv4hxg0o2v9nhm9UU3QI4/q2q7HYobrVQ/0yKS6lnyMn08/mDHa+KJrHQ/vhfToiW51PIIZqktDGJuPL4z2+KJjHY+x+TREN+2lEg6PcfaqATKZhMPVNbJOino1MJFImAp+NSAtsbSKx+cLbAQmpsTVI6mIwbiJf1Jo78LXMbKulrz7GIfEqCKRSIQYjJVLB5KEReKgqbV1jK/H/PkgfPgPr2yduK71TG0cTWTSd9KN7P1uBILsAyK0RyNnyn//cuTSCQiVL6LwzlnROysTEBXOKKHmJDLNA1ffHlGp/UL2wS4RlQOUkYmKbZn6c1R6DvWR1L6Nd6O990qlsszXJYnP8iRiYswSNYMiNT6Evutm7FuRkJ8jlog/3MJPyGf41KFfScOb1n3tCDUv9UMn/7hbUnshDWlO78tqet7KF33v1CwOweXKbBy5QVPdkF5XxGptxMmTJw8ePLh169ZqQwcNGsTn83fs2IHwAK+a2+PHj1VXR1MlJSWlqKjo2bNnoaGhCA8w0j4mJsbJycnMzExtaFRUVGZmplgsPnbs2PXr1xEGYKS9lkwPXL16VSgUwkZeXl5ISEh+fj6iOxhp/+jRIz8/jd+4gLVXDqtNTk4ODg5GdIfJ93IgWSi/qUaKj3AhclhYGKI1uGifm5sLZtzV1VVt6K1btzIyMlT3lJaWHjx4ENEaXPpwtRf2N2/elEqlkN3Nzc2tra25XO7hw4cR3cFFe+2F/c6dO8kNyO7Lli1bvHgxwgBcbL72fK/ExMTkwYMHqalY9Frh0q7n7+8PtXZyGgTtPH/+3MHBwcZGp6UlqzVY2Hxot6lXr54uwiP5LEs+CA+wsPk6GnwSsPmbNm1CGICF9todvUrUrFnzzJkzCAOYfF8ZFxeX5cuXQ5UP0R36l/dpaWlQcQf3TfdDGjRogDCA/vler0xPAl34Fy5cQHSH/trrVdiTWFlZ3blzB9Ed+tt8yPfdu3fX6xCIr2lsD52gufZisTg6Olrf8pvP5zs5OSG6Q3Ob/xEGn2TSpEnQi49oDc21/whHjwR686A1ENEamtt8yPf9+vVD+rNgwQLa93Qw+V49AoFA+0zcNIDO2kOrDggPFTakP+np6bQfskdn7R0dHaFjRnUgnu4kJiYWFBQgWkPz8t7DwyM+Pt7X1xfpSbNmzZo2bYpoDc3Le3d394SEBKQ/bDZbx/7+6gvNtSfzPdKfJUuWnD59GtEaRnv1pKSkQEc+ojU0N2tubm7gtSH9CQ0N1WMl0uoJ/ct70P4jWmloLzzCoQ/3I8x+VlZW586dEd2hv/Yf4epDw46zszOiO/Tvv/+IfA99vjjMvsHYfDVIJJL3V6imH4zNV8OqVauOHj2K6A6T79WQk5PDlPd0wMTExNraGvr0oGtHx0NWrFiBMACLbzP0NfuFhYU4fKKKhfZ6mX3o8+3WrdunX9Ly04Od9gEBAdojZ2Zment7Iwyg+ff3Xbt2LS4uLigoUOZjfD61/CB09vWgqgaql5aWslhvzZu9vb32oyA+VO7Nzc0R3aGzzZ81a1a9evWgoUa5B4zcBz+42bZtG+1n2CKheXk/f/58T09P5U/I9C1bttR+CPh64B8gDKD/fDuHDh0KCwsD4y+VSr28vOAnYlBAfz+/f//+YOcJBf7+/h+MD514qsUEjdHJ14t/li8te38sg0wx4b/qYhEaFhKQwWuvtNaFrnzokMoXVLegBRrce0bua35efn59tw5xjwvlixFUrE8hUzlKJvcHZHN+WBwSEqJmJQNCsQiHulsiyKesOK1eD6L2hjU/9TvPqzYa3KW5BdvRQ4A+xAds/oFf47PTJfBIkve6tdSvVqFOfF1XhNB8Bk0rY2hbMaPK+bglOMhDpYj4ZBaWJU+lbC5yb2j61bBaWiJq035vSJyoSPpFHwdHDwvEUK14eivn/oWspp0sW3XVOOJUo/Y7f45j81DvSZ6Iodqyb2VMLXd+z3HqV85Sb4mibuaUFkkZ4as77fs5Jr0UagpVr/2zO/km5swiqNUe5zrm4A89uJypNlS9ny8sJdh0/yIJE9hsVt4b9XMFqhdYLJLKpPTvxMSBMhG0aak34UzmxhdGe5pDII3DUBjtaY6isVJ9EKM9vjDa0x9NlXVGe/qjaTZ4Dd4/F7dVknFEQ/2+THNqYaALjM2nOVpa6BjtaQ7BJjRNIcJoT3OkEpmmEWhV5tH1H9B167bfUTXhv+tXxo4b1KFT86iox6gqWLtuxcjR35Dbvfp02r1nK6oK4uJi4CYfP36IPhZo12Mx7Xqq7D+wCxq81qze5OZG8zEK8JhSvdr12FyWTEznsdvFxUV+jZo2adwcYYx67SVlH9OHy+Fwjx77c9PmtTwez9e38Y9zFltZyuew7tr98+HDxg0cMIyMFvLr4tjY6M2b9sbHx44aMyB0/fbwrRvArDk6OA0cOBz0WLBwVnLyKx+fhlMmz/apJ1/upLCw8NDhvXfu3kxIiLWztW/Tpv2okRNNTEwgqHffgJEjJuTl5e7aHS4QCFo0bz35u1l2dho/vBKLxYFdWsFGQkLciZOH4eoNGzY6e+7UyVNH4uNjPDzqdOzQuV/fb5UdIJqCiouLly6f//DhXdjfq2fQ+xc6dvzg2bMnX6ckNW3Scsb0udbW8gV2b968dunyucdPHubn59X38R06dIwy/eUX5G/evO7vMyesrKybN/MfO2aKg0PlCQOgKNm3f8dva8Lr+zRE/zfqy3sW62O+Qb767z9FRYUrV2yYPeunyMiIHTs2ao/P5UITEgr9fRWkjEv/3G3o67dl6wYoOH8IXnTuzA0+j79+QwgZ8+ixA/v27xzwzdBlS9eOHz/tytULoLTyJH/+uZvFYh0/dnHXjiNPIiN27tqs5aIcDufyxXvu7p69vg6CDRD+n4tnV4b8XNfbZ9/ek2NGf3f4yL7QsNVkZC1Bq1YvgQS66teNS35eFZ8Qe+v2f6pXOXPmRE5O1oQJ38/78ZeIiHvwjEjxpR8kF6FQOOeHn+FBXF3d582fnp2dhRQpcs6PU99kZUIxBCk+IzN9ztypleb8gZvZsXPTgnnL9BReo5Lq871UCp0/eotvamo2dMhocvv6jauQunU5qlOnr5o2aQEbX7YLuHjx7NdfBzWoL5/2ul27TmEb18gUg/u/6T+kfbtObm7ln0pFRj66c/fG+HFTyZ/OzrWHDB4l3zK3gHwfHf0M6cPffx9v1KjJ99PmwLaNje3I4RNCVi0eMmgUbGsKkkgkl69c+CF4IXmrcCc3bv6rek6BqSlYIzID9ejRFxKNSCQCQ7U1/AAYJ8jZsB/yPRgeSKzwaJB0nj2L3LXjMCQICKpd2+3gob1ksiCJiLi/MmQRXKht2/ZIPzQOxK5KX+8z38bKbStLa5FQqMtRtWu7kxtmik9fPT3qkD8FJoKysjJ4ZXw+HzL33Xs3V6xcGBMbTeYGUEJ5hrp16yu3LSwswfYgnZFKpZFRj4YNHavc06RJC9gJCfeLzztoCrK1sUPyCVvf+on16jV4+fK58mfzZq2UlrNBg8/KDpRBnq7l5Ax+xtZtoRGP7mdlvSFDc3Nz4G9s7EtTU1NSePkTefvMn/sLkhd28jn8XyUlQEnaqeNXynKzSqhK7VUnHde9zFD9QPr9nyThWzZAFgRrD9kaSkGoTEK5+BHXeh9IW5DCtm0Pg3+q+3NysrUEkTOumgreLqoCKVU1DpjAt0GKaOCRsFnsadPHQPEPdhsSBNw26Xkg+QeghXy+iaabXLd+JaR4W1s7VKVQUMeTSPX72g3M/qnTR4L6DerRvQ+5h8wNVQLYYchwnQO7QxGjur+Wk4uWoIyMNNgoFZYqd0KGVo1TWlqi3CbtENh5cFMgPUFhD2YfVeR4EkgrJSXF8pF16pJ+l849wPNdvWZp8+atyPJRDwikX3kPNyCpuioej8eHB1P+TEpKRPoAma+kpMTevvz7Enh9lQrX/xMvr7oFhQVKfxsul5r6umZNBy1BpELgdtRTFDew/97926QnTxIT80K5/eLFU6j41LCvCb49FEmk8EjuGl9UxoHqDHiCL6KfkX7cq1cJa9Yum/LdbNKkQfoDt+Pu3ZtLl83fvu0gWXvSETgDS0MDnvrd8gXAq057sG/wnFBPg+09e7e9eZOh1+Hw4qAgPCOvLyWD5QRvCxyLgoL8j1sI533Gjp58/foVKEQg2z15ErF4yY8zZk2AFKYlqEaNmr6+fjt3boJ0DH77L0vnVSp3wPMHZw1cwuiXz8+dP93ui47gsnh6ekMxDzVGMOC379x48OAOGAPShECGBo81PHz9tf8u3713Cyo7mRnpSt+WJHj2QihVwelB+gAtOxINXbKfopceKtzgHPXs9SUUb0JhKfgsSE+ggDThm4wYGTRkWO9mTVuOGTMZfvbpF5CaloL+bz77rHH4pj+ggaFPv8BZwZPARP+yZA04mNqDoPWifn3fcRMGd+/ZDnJzt669lOPixOKy/kGDobU4oLP/jJnjIaXCG4D9nTp2gXrQ7j1b4D0cObJv6pTgwIBuUHdd89syEHVVSBg0qvy0cHbwD5NNBILly9ZVWkfq+gEAABAASURBVLTFzMxs4YIVt29fh0YUVBWo/x5v15IEaNvp970bYqjm7FkSW7+lRYdv1HyRqdnXo//cglgAeVuqoalGs/bV+bMcKJvnzvteU+jePcfJ1hXM0dCXwyGk1XkOcXk5Hb5PUygjPImGvhyxrLp/j+fkWAsxkHU8Qn35zYzboTkfVd4z0B1Ge3xhtKc5hL7t+Rw2wmJ2QSwgkF6+nlginxKOgQbIp4tkfD2GSqjXXt/REGfPH7G2ruKRBQyagI7Npo3boP8b9drL7YQ+jbpCYUn9+vUQwyfB1JSPqgIN+Z6lX19Ox45dzc2YeVc/EVKZSPfIivl29PH1wNHTa1p9CzPG4H862ARP98iK+XYYX4/hXRjt8YXRnuZAjU2/sZoEQjgsCooJ+s2vJyP+rw8eGIwIGaHn3IpMnqcLMs1aamrXQwy0R1O7HpPz6Y+G7+8JprinPxq+yZIxbj79Yer3NIcgmPnzcUUmY+bPxxj9xuuxOYRMwjh7dEA+VlODkup9PYlYJpF8Cmfv7r1bvfsGaInw+PHDlyrzGBiOc+dOF+g/nQc5Y1tcXIwukUtLSxf9/EOHTs23bA1Fnwow+JrmVqR4lvwWzVsdP/qPlgjrNqwUl5UhA5OTkx0atspMZZIcHYmJjebz+e7uOk3O+eDBncioRxfO3Ro7ZjIyAigu76dMGx0Y0O3rnv2+mzLSv2XbGzeuiiXiGjUcpkyeXcvJedLkEa9eJWzesn74sHEe7l5rflsWnxAL79rN1WP8uGk1azrcvnMjbOMaH5+G8XEx69dtmzl7om9Dv4iIex06dHZwcNq67fc/9hwnLzRwUI9pU35o3fqLCROHNvT1y8vNef48qrar+6iRE/k8fvCcyWw2Z8asCUuX/GZmpkcKePHiqXcdn1+Wzrt85YJ3nXqDBo38sr3cjG34fdXduzcFJgIzM3O4hK+v399nTmzbHsZms2cFT1oVEvYw4t7+/TtLSoolEkm3br179+oPR4E9SEtLychMd3Rwmjf3l/dPgqoU9fmezWGxPolFiIl54e3tA55ofHwMbK/6dePW8P1IboFPwd8e3ft4eXqvXRPepHHz9RtCrKysQ9dv3xS2x9TUbNXqJRAhOSkxJztrQP+h4Zv/MDExeZUYX1CQv3nT3oEDhsHZ6nr7kFfJL8hPT0+rV6+BVCpNfBXP4/Lmz1u6c8dh+Hn4yD5XV3c/v2ZdOveAC6kKv3jJj2CfVf8pZ0tWAtpnvskYPGjU2b+vt2nT7nfFzIsnTh5+9ixy2dK1cCdw2jlzpwqFwm5de7m7eX7TfwhcBUKXLps/btzUjWG75XeyazOUfUgxzU5CYlzIilAQXu1J0MegsZVOU3kvlRp+fH5iYjw8D2SX16+TYGPWrAXmiin2wMiTE46BRa1TRz4E9MmTiJu3rsHLAvk5HE779gGxcS/JCP6tPvf0lE/JB+oWFhUOJidZVAR5V2j/8uVzOzt7W1u75ORXkKjBiiDFjHD16tYnJ7uChFLHq26l2/tpwfLLF++p/tux7WClOC+in8LZvLy8wRo1bdISzlZcXLxl6wbIpi7O8tWnAwK6FhUVpaenwnZ09DMwErCxZVtor6+DyOliIeVB+ibnZoqLe9m3z0CBQKDlJPrzSeZW1Bd4FyAbaPD8xVNPjzqWFpbkfrDGQUGDkUKSjh26wAZYSHCUvu7VQXksOQ1h9MtnpJDyo15EgQbOtVzIn3BsUL9Bym0yHbyUG4P65ES8wJs3mZCYwF+Lj49VJhTdgVsCL69ly/Lh0m+y5GeDa4FOs4O/U41pbm6RmpYCSRNsD1wuMvLRd5NmKkNz83IsLa3y8nJTUl+T87lpOgmqUqjUXp41FfkA8qVXRbYDPeAd1VfMVQr7x4+VT5wqEgkDA7vNnbNY9XB49aAZaEn+hJRUx6t8nHhW1pvs7CxlVn4SGUHa/9jYaIuKFEbOqCkvHRT+mnJOSyVg86EUV90DPp1q1geDL58gtWLGMzDRjf2aCUVCBwfHA/tOVzrbv9cu1aoln7MPbhvKOHAyyP15+Xlg/z7zbQwZwMmxloVCYE0nqVo0zqWMDA9IS+Y21bIZdoITBzYAEgG8JkfFFAoeHnWePn0COQO2nz6LDPl1sUgkgpjgmTs6OpEHgvbKk5DT+ZE+C7zT+/dve1doD3aVnO3t4qVzRUWF7dsFJCUl1qzp+L6D80GbDwYfMjFIjhRJ9uKlsz179AOfFFJetGJ+1bS01HXrV5LzCSqfEeR3c/O4c/cGUlQR16xZ2rRJC0h58rRbpzztajpJ1aJxLuVPMOEOiAdFGnrXdL+ssM9gP2vUqAn+OTh3Hb4MzMrKHD0WykLT0tKSH4IX8Xg8udgqM+mCzR86ZAy57eLi2j9o8Jy508D1gw3IZx6KaXpfRD8bPWrSqDHfgLsHei9ftg6cO3jRKSnJ/fp3OXzwrF7dl4+fPBz07QhwQovBXReLJ06Y7ufXFPYv+XkVuHJwqoyMtBHDx9eu7UY+F9RByAMhQmjY6hMnDoERAiMPZTwivYGKtGtvX0PtST4CLW07GM2xlpmZMeDb7ufO3CDnbseEPUtifVpadvymxvtBVVneq10kRtMcsX36DLAw/6Sf8oCZgdyDlfAkBNLnuxwo7z+ijjds6BhkxIBPp5ygHR/0nm9HvnZCtZ5gTx1GnjQ/PUwfLr4w2uOLhvl2uISUmXCH7mhaB7vaz6vJ8EEYm09/9BuzxUAb5M3zGrrjNdTv2SxmeD49kFfXNTTVaKjfS6RMeU97GJuPLxrmXiAI5qMs2qNxfj3mY0zaw9h8fGG0xxf12vO4hJjx82kBiwNVfPXt8+p9Pb45IRUzDfp0gJAhW0f183Cq196vnUVxAaN9tSfuSY5Uhvy+sFUbql57r0Y25jacI+viEEN15sbpLG8/U02h2urxx35Pzkop9fvSzqelDWKoVtw5lx59r6BdP/uG/hoXgvxAG86xsKT0RJFELNNp+J7Wgd1ERRQtQOEkIz4+gpZmCULzpbUEaXwifYew63cedXvlg+h0uiT03cAr4psQPi3Mv+jtoCWmTu13JTklhSVsNQcTbz/2UmwTFfet5pwEOVOruv0KQRV/iXdiKH++3S8jT0SoTgKnvN77yUv1DAhVPnn45i0NfRu2bdMGVbqu8oRkUlO5LXLz7WnfXo5QTEf67tVR+SO9n2QJMvq7r7/89ZGv4913q3aOREIm/6/yXgmqUVunSdZ1qt8LbAQCOlr9AmGywNLbvpYe09HTCazb7UtLSzkKEJYwfTb4QvGcK9Qye/bs27dvI1zBuj2/sLAQ5+5K3Mt7Ho/3iaaXMT6Y8h5fsC7vx40b9/z5c4QrWJf3BQUF2Bp8xJT3fD4fW3ePKe/xBevyPigoKD09HeEK7vV7przHFKa8Z8p7TMG6vA8MDISsj3AF9/o9m81GuIK1zS8pKREIBAhXmPIeX/At76Gkh/IeYQy+5X2ZAoQx+Np8eHChUKhcRwFDmPIeX/At76El/8cff0QYg295Dy35Dx8+RBiDe3s+U94z4Ai+5X1xcXGXLl0QxuBb3nM4nPz8fIQxTHs+057PgB9Y99937NhRLBYjXMFae2jPF4lECFeY8p4p7xnwA2ub36dPnzdv3iBcwXq8Hjh6THmPKcz4fKa8xxSsy/sRI0bExsYiXMG6vJdIJEKhEOEKjjY/MDCQzWaD8GIFZAuPs7PzqVOnEE7gmO/Nzc2TkpJU95iYmID9R5iBY3kfFBRU6dNrJycnqOsjzMBR+0GDBrm4uCh/Qkd+7969MfwQH0ftoUI/dOhQqNmTPyEd9O3bF+EHpnU8sPBubm5IkQ66du1qZmaG8APf+v2wYcOgE8/V1bVXr14IS4y9jhfxb070vfz8XImoRCpfu0OGqvB+P7hMh35nUyx4wGIjvoBl7cBt8qWVRwNLZMQYr/aH1iVlJgkRgdhctokF38yGzzfnEjwuW+V+ZYqFe4kPLcWiQiW5FYeqPV6fk5bfjBiVScpK80RFuaWiIpFYJOVwCQ9f0y5DnZBRYozan96WkhBVzOGx7D2s7F2tUbUl5XlmbkohpCL/rrZNO9oiI8PotA+fGycRo9pNaphbmyJakBGbnZmQZ+PAGTTbHRkTxqV92KwYc3tTVz8HRDtibiZJxdJxyzyR0WBE2odOj3H5zNbayQrRlJhbyXy+bOhcd2QcGEsdTy58YzsaCw/UaeVSJiE2zYlBxoFRaL9pTqylo6l1TaOuEVUJns1dCBbrzzWJyAigXvujYclQU3NtRMMyXi31vnDLTCp7dicPUQ312qfGlHq0MtIasIGwdrG4cjgTUQ3F2u//NZFrwuHx8FqZ0qWBvUyKbv5N8fBwirXPTitz8jG6Rg8lv2749sipEGQABDaCqBsUm30qtb92PAMawC1q4NiH5tHUsbRIJtVpeXFDQaX28VFFXFMuwhWCjS4fykDUQeV4vcIciZWzoT6FlEjEZ/7Z9Cz6em5umoebXxv//g3qtSWDFi7v0qXTuKLi3POXtvJ5gnrerXp1nWFpaQ9BaRlxB44sTs+Mr+PZLKD9KGRIWBxW8stiRB1U5nupBFnYG6rR/tjpVddu7v/cv//cmcc/a9hx94E5jyMvkUFsNvfKf3sJgrX4x/PBUw/GJz46d3kLkn+iVbZ19/fWVjWDp/7ZvfNkiFNQYEB3TGDJK6N08n6KfT1Le4MU9mVlwnsRf3X8Ynjrln3NTK38m33dpFGXC1e2KSPY27oEtB8pEFhAdq9Xp1Xya/kKiU+eXs7NS/+663Qba0fHmp59eswqKS1ABoNvyhOXUdmgTpn2JYUG/AgyKeWZWCyqW8dfucfLvWlqekxRcblr7eJcXxkkEFiWCgth401WEo9rYmtT3thgaWFvbWXAFifwcyWUak9Zec/ncgz3CWRpiVzL37eOq7S/oDALzIBiU83Fi0vyefx3yiAux4AzL4KLz2JT+RkoZdqz+CwZAbm/VGBe9e+XdNyCev1ob1tbdb+NlaOWo0wFlkLhO85XqbAIGQxxqYTNxVJ7JE/1qCCzxBDa17Bz5XLlQ7DBXSf3FBRmQ281n6/NtbSxdiorK4WiwcmhDvx8nRqdX2DAlldRschEQKW/ReW14ckL3pQgAwAad+4w9sLlbXGJEWViEXj44TunHD39gRa6hvXbcTi8Q8eXi0SlefmZew/ONzU1YJ+yRCixdaAy71F57Zqu/OSXhqrldPhiaC2nupev7X4Ze9fExNy99mf9e83VfojAxHz0kDV/nQ+dv7QjOH1QzXvw+JzhjHKZSNroSyrHK1A5bkckEoXPeeUb6IHwIyU6M+914cSQOog6qLT50H1nZsmOu5uC8CM3pcilLsXTt1P8DXbrHnaXDmhr096ya1picqTaIGi1ZbPV3//Avj/51m+PqohL/+66dG232iAB37xE0TbwPpNGb6rl6K02KC+jSCaW9RzjgiiF+rGaO36OkyKOV0sUU3I5AAAB30lEQVRntaH5+W/EEvWtQKIyIY/LVxtkbmbL41VZriopKdDUwAdeoaYLWVrU4HDU91Q9v5LoWt+k24haiFKMYpxu6PQYt2Y1Leyw6MxNiEgrKxSOXUr9YG2jGKv55Td2rx5S2Zv5ySjIKi56U2IMwiMj0d63tU2jzy0iz8cjWiMpkyTeT58QYiz1GiP6NiPxWfHpbSl12jjzBTQcvpcWk/0mLm/Sak/jmeDDuL7Juncx69ZfOea2Ju7NaDVyN+ZGklgomRDihYwJY/wON3xurKhUZuVkWtu32g/aj7uXUpIrtHHkDprthowMI/3+/vpfGY+u5EvFiCNgW9qb2rhbCqpPQVCYXZz9uqA4WygWScwsWAGDatauZ46MD6Oed+P5/by753MLs8skYkSw5HMhsFiETPI2AkGoTMOhOlsCuV3+V2W6BeV+hBRHEm+PIlT2IMWUDIo9MmWciv1vT606CXN5BBm5weURtrX4Ad/WsLbnI2Ol2syrGfMoLydDXFoskYlV582A269QQKGx7F1FkGoaUDlMobNMoaoyaSimTJFJK8LRu9EVZyDKX1e58oqUR8hD5IHQxmhqxbJ3FtT2rh4zBzDzaOML1nMpYw6jPb4w2uMLoz2+MNrjC6M9vvwPAAD//8uWskMAAAAGSURBVAMAe5E77/qKjuUAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "analyst_instructions=\"\"\"You are tasked with creating a set of AI analyst personas. Follow these instructions carefully:\n",
    "\n",
    "1. First, review the research topic:\n",
    "{topic}\n",
    "        \n",
    "2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts: \n",
    "        \n",
    "{human_analyst_feedback}\n",
    "    \n",
    "3. Determine the most interesting themes based upon documents and / or feedback above.\n",
    "                    \n",
    "4. Pick the top {max_analysts} themes.\n",
    "\n",
    "5. Assign one analyst to each theme.\"\"\"\n",
    "\n",
    "# node 1\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    \n",
    "    \"\"\" Create analysts \"\"\"\n",
    "    \n",
    "    topic=state['topic']\n",
    "    max_analysts=state['max_analysts']\n",
    "    human_analyst_feedback=state.get('human_analyst_feedback', '')\n",
    "        \n",
    "    # Enforce structured output\n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    # System message\n",
    "    system_message = analyst_instructions.format(topic=topic,\n",
    "                                                            human_analyst_feedback=human_analyst_feedback, \n",
    "                                                            max_analysts=max_analysts)\n",
    "\n",
    "    # HumanMessage: Generate analysts question to kick off the chain\n",
    "    analysts = structured_llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=\"Generate the set of analysts.\")])\n",
    "    \n",
    "    # Write the list of analysis to state\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "\n",
    "# Node 2 - No-op for human feedback\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\" No-op node that should be interrupted on \"\"\"\n",
    "    pass\n",
    "\n",
    "# conditional edge\n",
    "# is there a human feedback in the state? yes -> go back to creat_analysts and generate \n",
    "# based upon the feedback. no -> end analyst generation\n",
    "def should_continue(state: GenerateAnalystsState):\n",
    "    \"\"\" Return the next node to execute \"\"\"\n",
    "\n",
    "    # Check if human feedback\n",
    "    human_analyst_feedback=state.get('human_analyst_feedback', None)\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "    \n",
    "    # Otherwise end\n",
    "    return END\n",
    "\n",
    "# Add nodes and edges \n",
    "builder = StateGraph(GenerateAnalystsState)\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\"human_feedback\", should_continue, [\"create_analysts\", END])\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ba325",
   "metadata": {},
   "source": [
    "#### This is phase 1 of research: Make sure the scope is set correctly before do all the work.\n",
    "\n",
    "##### Human feedback does that by reviewing the generated team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3410e2d4",
   "metadata": {},
   "source": [
    "## Executing Analyst Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c22cb05-c436-4358-8f7a-72d722f9b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Carter\n",
      "Affiliation: Tech Innovations Inc.\n",
      "Role: AI Framework Specialist\n",
      "Description: Dr. Carter focuses on the technical advantages of adopting LangGraph, emphasizing its scalability and integration capabilities with existing systems. She is motivated by the potential for LangGraph to enhance AI-driven solutions in various industries.\n",
      "--------------------------------------------------\n",
      "Name: Mr. James Liu\n",
      "Affiliation: Data Science Research Group\n",
      "Role: Data Analyst\n",
      "Description: Mr. Liu analyzes the impact of LangGraph on data processing efficiency and accuracy. His work is driven by a desire to improve data-driven decision-making processes and to showcase how LangGraph can streamline workflows in data-intensive environments.\n",
      "--------------------------------------------------\n",
      "Name: Ms. Sarah Thompson\n",
      "Affiliation: Business Strategy Consultants\n",
      "Role: Business Development Analyst\n",
      "Description: Ms. Thompson evaluates the business implications of adopting LangGraph, focusing on cost-effectiveness and competitive advantage. She aims to highlight how LangGraph can transform business operations and drive innovation in the marketplace.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "max_analysts = 3 \n",
    "topic = \"The benefits of adopting LangGraph as an agent framework\"\n",
    "\n",
    "# define thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph (until the first interruption)\n",
    "for event in graph.stream({\"topic\":topic,\"max_analysts\":max_analysts,}, thread, stream_mode=\"values\"):\n",
    "    # Review\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81ad23-5656-43e6-b50a-0d7a4f69a60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get state and look at next node\n",
    "state = graph.get_state(thread)\n",
    "state.next\n",
    "\n",
    "## As expected, we're waiting on human feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72b2a402-fd10-4f26-9a32-3e3c0d4aaf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f09bad4-e17e-6253-8002-1c7743432867'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now update the state as if we are the human_feedback node\n",
    "# hardcoding a human feedback for the demo\n",
    "graph.update_state(thread, {\"human_analyst_feedback\": \n",
    "                            \"Add in someone from a startup to add an entrepreneur perspective\"}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8816eb9-9906-441b-b552-be71107db14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Carter\n",
      "Affiliation: Tech Innovations Inc.\n",
      "Role: AI Framework Specialist\n",
      "Description: Dr. Carter focuses on the technical advantages of adopting LangGraph, emphasizing its scalability and integration capabilities with existing systems. She is motivated by the potential for LangGraph to enhance AI-driven solutions in various industries.\n",
      "--------------------------------------------------\n",
      "Name: Mr. James Liu\n",
      "Affiliation: Data Science Research Group\n",
      "Role: Data Analyst\n",
      "Description: Mr. Liu analyzes the impact of LangGraph on data processing efficiency and accuracy. His work is driven by a desire to improve data-driven decision-making processes and to showcase how LangGraph can streamline workflows in data-intensive environments.\n",
      "--------------------------------------------------\n",
      "Name: Ms. Sarah Thompson\n",
      "Affiliation: Business Strategy Consultants\n",
      "Role: Business Development Analyst\n",
      "Description: Ms. Thompson evaluates the business implications of adopting LangGraph, focusing on cost-effectiveness and competitive advantage. She aims to highlight how LangGraph can transform business operations and drive innovation in the marketplace.\n",
      "--------------------------------------------------\n",
      "Name: Jordan Lee\n",
      "Affiliation: Tech Startup Incubator\n",
      "Role: Startup Founder\n",
      "Description: Jordan is the founder of a tech startup focused on AI solutions. They are interested in how LangGraph can streamline development processes and enhance product offerings, particularly in the context of limited resources and rapid iteration.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Emily Chen\n",
      "Affiliation: AI Research Institute\n",
      "Role: AI Researcher\n",
      "Description: Dr. Chen is a leading researcher in AI frameworks and their applications. She focuses on the technical advantages of LangGraph, including its scalability and integration capabilities, and how these can benefit both academic and commercial projects.\n",
      "--------------------------------------------------\n",
      "Name: Michael Thompson\n",
      "Affiliation: Enterprise Software Company\n",
      "Role: Product Manager\n",
      "Description: Michael works as a product manager at a large enterprise software company. He is concerned with the practical implementation of LangGraph in existing systems, its impact on team collaboration, and the potential for improving efficiency in large-scale projects.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Continue the graph execution (pass None)\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    # Review\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50) \n",
    "\n",
    "### 3 new analysts added based upon the human feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a43ac322-5926-4932-8653-68206fec0d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f09bad8-38da-6449-8004-067bd8ca45d2'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we are satisfied, then we simply supply no feedback (pass None)\n",
    "further_feedack = None\n",
    "graph.update_state(thread, {\"human_analyst_feedback\": \n",
    "                            further_feedack}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab034e65-aeee-4723-8d6d-74541b548425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the graph execution to end\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f204e8a-285c-4e46-8223-a695caec7764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Analyst(affiliation='Tech Startup Incubator', name='Jordan Lee', role='Startup Founder', description='Jordan is the founder of a tech startup focused on AI solutions. They are interested in how LangGraph can streamline development processes and enhance product offerings, particularly in the context of limited resources and rapid iteration.'),\n",
       " Analyst(affiliation='AI Research Institute', name='Dr. Emily Chen', role='AI Researcher', description='Dr. Chen is a leading researcher in AI frameworks and their applications. She focuses on the technical advantages of LangGraph, including its scalability and integration capabilities, and how these can benefit both academic and commercial projects.'),\n",
       " Analyst(affiliation='Enterprise Software Company', name='Michael Thompson', role='Product Manager', description='Michael works as a product manager at a large enterprise software company. He is concerned with the practical implementation of LangGraph in existing systems, its impact on team collaboration, and the potential for improving efficiency in large-scale projects.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state = graph.get_state(thread)\n",
    "analysts = final_state.values.get('analysts')\n",
    "analysts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59704086-cb3b-42e9-8395-37be6f0d44e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state.next\n",
    "\n",
    "## Graph is done, no next node to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95717ba3-aa00-48d6-bbb7-5fe4db5919bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Jordan Lee\n",
      "Affiliation: Tech Startup Incubator\n",
      "Role: Startup Founder\n",
      "Description: Jordan is the founder of a tech startup focused on AI solutions. They are interested in how LangGraph can streamline development processes and enhance product offerings, particularly in the context of limited resources and rapid iteration.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Emily Chen\n",
      "Affiliation: AI Research Institute\n",
      "Role: AI Researcher\n",
      "Description: Dr. Chen is a leading researcher in AI frameworks and their applications. She focuses on the technical advantages of LangGraph, including its scalability and integration capabilities, and how these can benefit both academic and commercial projects.\n",
      "--------------------------------------------------\n",
      "Name: Michael Thompson\n",
      "Affiliation: Enterprise Software Company\n",
      "Role: Product Manager\n",
      "Description: Michael works as a product manager at a large enterprise software company. He is concerned with the practical implementation of LangGraph in existing systems, its impact on team collaboration, and the potential for improving efficiency in large-scale projects.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for analyst in analysts:\n",
    "    print(f\"Name: {analyst.name}\")\n",
    "    print(f\"Affiliation: {analyst.affiliation}\")\n",
    "    print(f\"Role: {analyst.role}\")\n",
    "    print(f\"Description: {analyst.description}\")\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2498e4-20ae-4503-9dd0-a4165132b7a7",
   "metadata": {},
   "source": [
    "## Conduct Interview\n",
    "\n",
    "### Generate Question\n",
    "\n",
    "Each generated analyst will ask questions to the expert.\n",
    "- The expert will be a system that has access to some sources that we give it to answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5f559-f42e-442b-87cd-dbf0a91abf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import  Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class InterviewState(MessagesState): # State for the interview\n",
    "    max_num_turns: int # Max number of turns of conversation in the interview (so we don't get stuck in an infinite loop)\n",
    "    context: Annotated[list, operator.add] # Aggregated source docs, paralellized search (wikipedia and tavily)\n",
    "    analyst: Analyst # Analyst asking questions\n",
    "    interview: str # Interview transcription\n",
    "    sections: list # Final key we duplicate in outer state for Send() API. The report will have these sections\n",
    "\n",
    "class SearchQuery(BaseModel): # schema to perform search queries with the expert on the resources, for structured output\n",
    "    search_query: str = Field(None, description=\"Search query for retrieval.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c2e71eb-07ad-4bea-aabc-dbaf551408c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
    "\n",
    "Your goal is boil down to interesting and specific insights related to your topic.\n",
    "\n",
    "1. Interesting: Insights that people will find surprising or non-obvious.\n",
    "        \n",
    "2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
    "\n",
    "Here is your topic of focus and set of goals: {goals}\n",
    "        \n",
    "Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
    "\n",
    "Continue to ask questions to drill down and refine your understanding of the topic.\n",
    "        \n",
    "When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
    "\n",
    "Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\"\n",
    "\n",
    "# Node 1\n",
    "def generate_question(state: InterviewState):\n",
    "    \"\"\" Node to generate a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"] # built-in from MessagesState\n",
    "\n",
    "    # Generate question and set goals as the analyst persona\n",
    "    # persona is a concatenation of the analysts identity and goal (Analyst Class: def persona(self) -> str)\n",
    "    system_message = question_instructions.format(goals=analyst.persona)\n",
    "    question = llm.invoke([SystemMessage(content=system_message)]+messages)\n",
    "        \n",
    "    # Write the analyst question to the state\n",
    "    return {\"messages\": [question]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ff33a-6232-4a79-8a82-882a645394f5",
   "metadata": {},
   "source": [
    "### Generate Answer: Parallelization\n",
    "\n",
    "##### The expert will gather information **from multiple sources in parallel** to answer questions.\n",
    "\n",
    "For example, we can use:\n",
    "\n",
    "* Specific web sites e.g., via [`WebBaseLoader`](https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/)\n",
    "* Indexed documents e.g., via [RAG](https://python.langchain.com/v0.2/docs/tutorials/rag/)\n",
    "* Web search\n",
    "* Wikipedia search\n",
    "\n",
    "You can try different web search tools, like [Tavily](https://tavily.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "606ea95b-e811-4299-8b66-835d4016c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c61ae74a-f838-4e97-8bd5-48ccd15b7789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_489631/2338646555.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(max_results=3)\n"
     ]
    }
   ],
   "source": [
    "# Web search tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d8f760b-5a1a-4fa9-a014-d3fb02bec51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia search tool\n",
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb1603",
   "metadata": {},
   "source": [
    "##### Now, we create nodes to search the web and wikipedia.\n",
    "\n",
    "##### We'll also create a node to answer analyst questions.\n",
    "\n",
    "##### Finally, we'll create nodes to save the full interview and to write a summary (\"section\") of the interview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c863768-2278-415b-aef1-96fd18c1b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "# Search query formulation\n",
    "search_instructions = SystemMessage(content=f\"\"\"You will be given a conversation between an analyst and an expert. \n",
    "\n",
    "Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.\n",
    "        \n",
    "First, analyze the full conversation.\n",
    "\n",
    "Pay particular attention to the final question posed by the analyst.\n",
    "\n",
    "Convert this final question into a well-structured web search query\"\"\")\n",
    "\n",
    "# node 2\n",
    "def search_web(state: InterviewState):\n",
    "    \n",
    "    \"\"\" Retrieve docs from web search \"\"\"\n",
    "\n",
    "    # Search query\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "    # generate the formulated search query\n",
    "    # this way we avoid sending the entire conversation to the LLM, we just send \n",
    "    # the formulated search query for the tool search!\n",
    "    search_query = structured_llm.invoke([search_instructions]+state['messages'])\n",
    "    \n",
    "    # Search\n",
    "    search_docs = tavily_search.invoke(search_query.search_query)\n",
    "\n",
    "     # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]} \n",
    "\n",
    "# Node 3\n",
    "def search_wikipedia(state: InterviewState):\n",
    "    \n",
    "    \"\"\" Retrieve docs from wikipedia \"\"\"\n",
    "\n",
    "    # Search query\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "    # generate the formulated search query\n",
    "    search_query = structured_llm.invoke([search_instructions]+state['messages'])\n",
    "    \n",
    "    # Search\n",
    "    search_docs = WikipediaLoader(query=search_query.search_query, \n",
    "                                  load_max_docs=2).load()\n",
    "\n",
    "     # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]} \n",
    "\n",
    "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
    "\n",
    "Here is analyst area of focus: {goals}. \n",
    "        \n",
    "You goal is to answer a question posed by the interviewer.\n",
    "\n",
    "To answer question, use this context:\n",
    "        \n",
    "{context}\n",
    "\n",
    "When answering questions, follow these guidelines:\n",
    "        \n",
    "1. Use only the information provided in the context. \n",
    "        \n",
    "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "\n",
    "3. The context contain sources at the topic of each individual document.\n",
    "\n",
    "4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
    "\n",
    "5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
    "        \n",
    "6. If the source is: <Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>' then just list: \n",
    "        \n",
    "[1] assistant/docs/llama3_1.pdf, page 7 \n",
    "        \n",
    "And skip the addition of the brackets as well as the Document source preamble in your citation.\"\"\"\n",
    "\n",
    "# Node 4\n",
    "def generate_answer(state: InterviewState):\n",
    "    \n",
    "    \"\"\" Node to answer a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Answer question\n",
    "    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n",
    "    answer = llm.invoke([SystemMessage(content=system_message)]+messages)\n",
    "            \n",
    "    # Name the message as coming from the expert\n",
    "    answer.name = \"expert\"\n",
    "    \n",
    "    # Append it to state\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "# Node 5\n",
    "def save_interview(state: InterviewState):\n",
    "    \n",
    "    \"\"\" Save interviews \"\"\"\n",
    "\n",
    "    # Get messages\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Convert interview to a string\n",
    "    interview = get_buffer_string(messages)\n",
    "    \n",
    "    # Save to interviews key\n",
    "    return {\"interview\": interview}\n",
    "\n",
    "# Conditional edge\n",
    "# If the expert has answered more than the max turns or the analyst signals the end \n",
    "# of it with the message \"Thank you so much for your help\", end the interview.\n",
    "# Else, ask another questio\n",
    "def route_messages(state: InterviewState, \n",
    "                   name: str = \"expert\"):\n",
    "\n",
    "    \"\"\" Route between question and answer \"\"\"\n",
    "    \n",
    "    # Get messages\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get('max_num_turns',2)\n",
    "\n",
    "    # Check the number of expert answers \n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "\n",
    "    # End if expert has answered more than the max turns\n",
    "    if num_responses >= max_num_turns:\n",
    "        return 'save_interview'\n",
    "\n",
    "    # This router is run after each question - answer pair \n",
    "    # Get the last question asked to check if it signals the end of discussion\n",
    "    last_question = messages[-2]\n",
    "    \n",
    "    if \"Thank you so much for your help\" in last_question.content: # Dummy logic for demo purposes\n",
    "        return 'save_interview'\n",
    "    return \"ask_question\"\n",
    "\n",
    "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
    "            \n",
    "Your task is to create a short, easily digestible section of a report based on a set of source documents.\n",
    "\n",
    "1. Analyze the content of the source documents: \n",
    "- The name of each source document is at the start of the document, with the <Document tag.\n",
    "        \n",
    "2. Create a report structure using markdown formatting:\n",
    "- Use ## for the section title\n",
    "- Use ### for sub-section headers\n",
    "        \n",
    "3. Write the report following this structure:\n",
    "a. Title (## header)\n",
    "b. Summary (### header)\n",
    "c. Sources (### header)\n",
    "\n",
    "4. Make your title engaging based upon the focus area of the analyst: \n",
    "{focus}\n",
    "\n",
    "5. For the summary section:\n",
    "- Set up summary with general background / context related to the focus area of the analyst\n",
    "- Emphasize what is novel, interesting, or surprising about insights gathered from the interview\n",
    "- Create a numbered list of source documents, as you use them\n",
    "- Do not mention the names of interviewers or experts\n",
    "- Aim for approximately 400 words maximum\n",
    "- Use numbered sources in your report (e.g., [1], [2]) based on information from source documents\n",
    "        \n",
    "6. In the Sources section:\n",
    "- Include all sources used in your report\n",
    "- Provide full links to relevant websites or specific document paths\n",
    "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
    "- It will look like:\n",
    "\n",
    "### Sources\n",
    "[1] Link or Document name\n",
    "[2] Link or Document name\n",
    "\n",
    "7. Be sure to combine sources. For example this is not correct:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "There should be no redundant sources. It should simply be:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "        \n",
    "8. Final review:\n",
    "- Ensure the report follows the required structure\n",
    "- Include no preamble before the title of the report\n",
    "- Check that all guidelines have been followed\"\"\"\n",
    "\n",
    "# Node 6\n",
    "def write_section(state: InterviewState):\n",
    "\n",
    "    \"\"\" Node to answer a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    interview = state[\"interview\"] # Not used in this example, we're using the context instead\n",
    "    context = state[\"context\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "   \n",
    "    # Write section using either the gathered source docs from interview (context) or the interview itself (interview)\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "    section = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=f\"Use this source to write your section: {context}\")]) \n",
    "                \n",
    "    # Append it to state\n",
    "    return {\"sections\": [section.content]}\n",
    "\n",
    "# Add nodes and edges \n",
    "interview_builder = StateGraph(InterviewState)\n",
    "interview_builder.add_node(\"ask_question\", generate_question)\n",
    "interview_builder.add_node(\"search_web\", search_web)\n",
    "interview_builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
    "interview_builder.add_node(\"answer_question\", generate_answer)\n",
    "interview_builder.add_node(\"save_interview\", save_interview)\n",
    "interview_builder.add_node(\"write_section\", write_section)\n",
    "\n",
    "# Flow\n",
    "interview_builder.add_edge(START, \"ask_question\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_web\") # branch 1 fan-out\n",
    "interview_builder.add_edge(\"ask_question\", \"search_wikipedia\") # branch 2 fan-out\n",
    "interview_builder.add_edge(\"search_web\", \"answer_question\") # branch 1 fan-in\n",
    "interview_builder.add_edge(\"search_wikipedia\", \"answer_question\") # branch 2 fan-in\n",
    "interview_builder.add_conditional_edges(\"answer_question\", route_messages,['ask_question','save_interview']) # interview flow until save_interview in the end of it\n",
    "interview_builder.add_edge(\"save_interview\", \"write_section\")\n",
    "interview_builder.add_edge(\"write_section\", END)\n",
    "\n",
    "# Interview \n",
    "memory = MemorySaver()\n",
    "interview_graph = interview_builder.compile(checkpointer=memory).with_config(run_name=\"Conduct Interviews\")\n",
    "\n",
    "# View - got error from mermaid api...\n",
    "# display(Image(interview_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750ac4f-f458-4b2d-8bad-32ce34895758",
   "metadata": {},
   "source": [
    "#### Here, we run the interview passing an index of the llama3.1 paper, which is related to our topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f382f1-6e93-48d0-a44a-1094d26ccb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analyst(affiliation='Tech Startup Incubator', name='Jordan Lee', role='Startup Founder', description='Jordan is the founder of a tech startup focused on AI solutions. They are interested in how LangGraph can streamline development processes and enhance product offerings, particularly in the context of limited resources and rapid iteration.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick one analyst\n",
    "analysts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb87ff97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The benefits of adopting LangGraph as an agent framework'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2242d4e-8430-4de9-8cf7-3ad2f9a22b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Revolutionizing AI Development with LangGraph\n",
       "\n",
       "### Summary\n",
       "In the rapidly evolving landscape of AI solutions, LangGraph emerges as a transformative framework that significantly enhances the development of stateful, multi-agent applications. Unlike traditional linear frameworks, LangGraph supports cyclical workflows, allowing agents to revisit and refine their actions based on new information. This capability is crucial for creating sophisticated AI systems that can engage in complex reasoning and adapt to changing conditions, making it particularly valuable for startups like Jordan's, which operate under resource constraints and require rapid iteration.\n",
       "\n",
       "One of the most striking insights from the analysis of LangGraph is its ability to maintain conversational context across interactions, which is essential for applications such as virtual assistants and customer service bots. The framework's state management capabilities ensure that agents can handle diverse user inputs effectively, leading to more meaningful dialogues and improved user experiences [1]. Furthermore, LangGraph's structured approach simplifies the development of complex workflows, enabling agents to perform tasks autonomously and interact with other systems dynamically [2].\n",
       "\n",
       "The framework's integration with the LangChain ecosystem allows for seamless collaboration among multiple agents, enhancing operational efficiency in scenarios like supply chain management [3]. This multi-agent coordination not only streamlines processes but also fosters a more adaptable AI environment, capable of handling non-linear business processes [4]. Notably, companies like Uber have successfully implemented LangGraph for automated unit test generation, showcasing its potential to reduce development time while improving code quality through iterative testing and refinement cycles [5].\n",
       "\n",
       "As AI applications continue to grow in sophistication, the insights gathered highlight LangGraph's role as a critical tool for developers aiming to create flexible and reliable AI systems. Its unique features, such as fine-grained control over agent workflows and the ability to integrate external APIs, position it as a game-changer in the field of AI development [6]. \n",
       "\n",
       "### Sources\n",
       "[1] https://medium.com/@ken_lin/langgraph-a-framework-for-building-stateful-multi-agent-llm-applications-a51d5eb68d03  \n",
       "[2] https://www.datacamp.com/tutorial/langgraph-tutorial  \n",
       "[3] https://blog.agen.cy/p/agency-revolutionizing-ai-development  \n",
       "[4] https://www.linkedin.com/pulse/reflection-agents-langgraph-building-self-improving-ai-prasanna-btdgc  \n",
       "[5] https://medium.com/@mariumaslam499/build-your-own-ai-coding-agent-with-langgraph-040644343e73  \n",
       "[6] https://activewizards.com/blog/a-deep-dive-into-langgraph-for-self-correcting-ai-agents  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Executing for one analyst, for testing!\n",
    "from IPython.display import Markdown\n",
    "messages = [HumanMessage(f\"So you said you were writing an article on {topic}?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "interview = interview_graph.invoke({\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 2}, thread)\n",
    "Markdown(interview['sections'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b739e87-68bb-4e96-a86a-704e84240a6c",
   "metadata": {},
   "source": [
    "### Parallelze interviews: Map-Reduce\n",
    "\n",
    "##### We parallelize the interviews via the `Send()` API, a **map step**.\n",
    "\n",
    "##### We combine them into the report body in a **reduce step**.\n",
    "\n",
    "### Finalize\n",
    "\n",
    "##### We add a final step to write an intro and conclusion to the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a0042f9-5b9f-441a-9e8d-7d8189f44140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class ResearchGraphState(TypedDict):\n",
    "    topic: str # Research topic\n",
    "    max_analysts: int # Number of analysts\n",
    "    human_analyst_feedback: str # Human feedback for analysts generation\n",
    "    analysts: List[Analyst] # Analysts asking questions\n",
    "    sections: Annotated[list, operator.add] # Parallellized -> Send() API key\n",
    "    introduction: str # Introduction for the final report\n",
    "    content: str # Content for the final report\n",
    "    conclusion: str # Conclusion for the final report\n",
    "    final_report: str # Final report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed38e6",
   "metadata": {},
   "source": [
    "## Execute for all analysts and get the full report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2224592-d2ff-469d-97bd-928809f896d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_489631/4083986792.py:1: LangGraphDeprecatedSinceV10: Importing Send from langgraph.constants is deprecated. Please use 'from langgraph.types import Send' instead. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  from langgraph.constants import Send\n"
     ]
    }
   ],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "# Node\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    \"\"\" This is the \"map\" step where we run each interview sub-graph using Send API \"\"\"    \n",
    "\n",
    "    # Check if human feedback\n",
    "    human_analyst_feedback=state.get('human_analyst_feedback')\n",
    "    if human_analyst_feedback:\n",
    "        # Return to create_analysts\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    # Otherwise kick off interviews in parallel via Send() API (Map-reduce)\n",
    "    else:\n",
    "        topic = state[\"topic\"]\n",
    "        return [Send(\"conduct_interview\", {\"analyst\": analyst,\n",
    "                                           \"messages\": [HumanMessage(\n",
    "                                               content=f\"So you said you were writing an article on {topic}?\"\n",
    "                                           )\n",
    "                                                       ]}) for analyst in state[\"analysts\"]]\n",
    "\n",
    "report_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic: \n",
    "\n",
    "{topic}\n",
    "    \n",
    "You have a team of analysts. Each analyst has done two things: \n",
    "\n",
    "1. They conducted an interview with an expert on a specific sub-topic.\n",
    "2. They write up their finding into a memo.\n",
    "\n",
    "Your task: \n",
    "\n",
    "1. You will be given a collection of memos from your analysts.\n",
    "2. Think carefully about the insights from each memo.\n",
    "3. Consolidate these into a crisp overall summary that ties together the central ideas from all of the memos. \n",
    "4. Summarize the central points in each memo into a cohesive single narrative.\n",
    "\n",
    "To format your report:\n",
    " \n",
    "1. Use markdown formatting. \n",
    "2. Include no pre-amble for the report.\n",
    "3. Use no sub-heading. \n",
    "4. Start your report with a single title header: ## Insights\n",
    "5. Do not mention any analyst names in your report.\n",
    "6. Preserve any citations in the memos, which will be annotated in brackets, for example [1] or [2].\n",
    "7. Create a final, consolidated list of sources and add to a Sources section with the `## Sources` header.\n",
    "8. List your sources in order and do not repeat.\n",
    "\n",
    "[1] Source 1\n",
    "[2] Source 2\n",
    "\n",
    "Here are the memos from your analysts to build your report from: \n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "# Node\n",
    "def write_report(state: ResearchGraphState):\n",
    "    # Full set of sections\n",
    "    sections = state[\"sections\"]\n",
    "\n",
    "    # Get topic\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Concat all sections together\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "    \n",
    "    # Summarize the sections into a final report\n",
    "    system_message = report_writer_instructions.format(topic=topic, context=formatted_str_sections)    \n",
    "    report = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=f\"Write a report based upon these memos.\")]) \n",
    "    return {\"content\": report.content}\n",
    "\n",
    "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
    "\n",
    "You will be given all of the sections of the report.\n",
    "\n",
    "You job is to write a crisp and compelling introduction or conclusion section.\n",
    "\n",
    "The user will instruct you whether to write the introduction or conclusion.\n",
    "\n",
    "Include no pre-amble for either section.\n",
    "\n",
    "Target around 100 words, crisply previewing (for introduction) or recapping (for conclusion) all of the sections of the report.\n",
    "\n",
    "Use markdown formatting. \n",
    "\n",
    "For your introduction, create a compelling title and use the # header for the title.\n",
    "\n",
    "For your introduction, use ## Introduction as the section header. \n",
    "\n",
    "For your conclusion, use ## Conclusion as the section header.\n",
    "\n",
    "Here are the sections to reflect on for writing: {formatted_str_sections}\"\"\"\n",
    "\n",
    "# Node\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    # Full set of sections\n",
    "    sections = state[\"sections\"]\n",
    "\n",
    "    # Get topic\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Concat all sections together\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "    \n",
    "    # Summarize the sections into a final report\n",
    "    \n",
    "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)    \n",
    "    intro = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report introduction\")]) \n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "# Node\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    # Full set of sections\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Concat all sections together\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "    \n",
    "    # Summarize the sections into a final report\n",
    "    \n",
    "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)    \n",
    "    conclusion = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report conclusion\")]) \n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "# REDUCE PHASE NODE\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    \"\"\" The is the \"reduce\" step where we gather all the sections, combine them, and reflect on them to write the intro/conclusion \"\"\"\n",
    "    # Save full final report\n",
    "    content = state[\"content\"]\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    final_report = state[\"introduction\"] + \"\\n\\n---\\n\\n\" + content + \"\\n\\n---\\n\\n\" + state[\"conclusion\"]\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "    return {\"final_report\": final_report}\n",
    "\n",
    "# Add nodes and edges \n",
    "builder = StateGraph(ResearchGraphState)\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "\n",
    "# create an interview as a subgraph (it has its own state, but we dont want all that in the overall state)\n",
    "# So we'll encapsulate it in a subgraph\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile()) \n",
    "\n",
    "builder.add_node(\"write_report\",write_report)\n",
    "builder.add_node(\"write_introduction\",write_introduction)\n",
    "builder.add_node(\"write_conclusion\",write_conclusion)\n",
    "builder.add_node(\"finalize_report\",finalize_report)\n",
    "\n",
    "# Logic\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"])\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "builder.add_edge([\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\")\n",
    "builder.add_edge(\"finalize_report\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n",
    "#display(Image(graph.get_graph(xray=1).draw_mermaid_png())) # error mermaid api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64ba9a-2b5e-40e1-a778-0f635aa3f6d0",
   "metadata": {},
   "source": [
    "#### Let's ask an open-ended question about LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "362932ee-4106-4a2d-a32d-b812eafcf9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Carter\n",
      "Affiliation: Tech Innovations Inc.\n",
      "Role: AI Framework Specialist\n",
      "Description: Dr. Carter focuses on the technical advantages of adopting LangGraph, emphasizing its scalability and integration capabilities with existing systems. She is motivated by the potential for LangGraph to enhance AI-driven solutions in various industries.\n",
      "--------------------------------------------------\n",
      "Name: Mr. James Liu\n",
      "Affiliation: Data Science Research Group\n",
      "Role: Data Analyst\n",
      "Description: Mr. Liu analyzes the impact of LangGraph on data processing efficiency and accuracy. His work is driven by a desire to improve data-driven decision-making processes and to showcase how LangGraph can streamline workflows in data-intensive environments.\n",
      "--------------------------------------------------\n",
      "Name: Ms. Sarah Thompson\n",
      "Affiliation: Business Strategy Consultants\n",
      "Role: Business Analyst\n",
      "Description: Ms. Thompson evaluates the business implications of adopting LangGraph, focusing on cost-effectiveness and return on investment. She aims to provide insights into how organizations can leverage LangGraph to gain a competitive edge in their respective markets.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "max_analysts = 3 \n",
    "topic = \"The benefits of adopting LangGraph as an agent framework\"\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream({\"topic\":topic,\n",
    "                           \"max_analysts\":max_analysts}, \n",
    "                          thread, \n",
    "                          stream_mode=\"values\"):\n",
    "    \n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac521a5f-5a4f-44f9-8af9-d05228e20882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f09bc01-6db8-6d25-8002-59229d0539c8'}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now update the state as if we are the human_feedback node\n",
    "graph.update_state(thread, {\"human_analyst_feedback\": \n",
    "                                \"Add in the CEO of gen ai native startup\"}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3be311f-62ee-49e7-b037-75c53d8960a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Carter\n",
      "Affiliation: Tech Innovations Inc.\n",
      "Role: AI Framework Specialist\n",
      "Description: Dr. Carter focuses on the technical advantages of adopting LangGraph, emphasizing its scalability and integration capabilities with existing systems. She is motivated by the potential for LangGraph to enhance AI-driven solutions in various industries.\n",
      "--------------------------------------------------\n",
      "Name: Mr. James Liu\n",
      "Affiliation: Data Science Research Group\n",
      "Role: Data Analyst\n",
      "Description: Mr. Liu analyzes the impact of LangGraph on data processing efficiency and accuracy. His work is driven by a desire to improve data-driven decision-making processes and to showcase how LangGraph can streamline workflows in data-intensive environments.\n",
      "--------------------------------------------------\n",
      "Name: Ms. Sarah Thompson\n",
      "Affiliation: Business Strategy Consultants\n",
      "Role: Business Analyst\n",
      "Description: Ms. Thompson evaluates the business implications of adopting LangGraph, focusing on cost-effectiveness and return on investment. She aims to provide insights into how organizations can leverage LangGraph to gain a competitive edge in their respective markets.\n",
      "--------------------------------------------------\n",
      "Name: Alexandra Chen\n",
      "Affiliation: Gen AI Native Startup\n",
      "Role: CEO\n",
      "Description: As the CEO of a startup focused on generative AI, Alexandra is keen on exploring how LangGraph can streamline agent frameworks to enhance productivity and innovation in AI applications. She is particularly interested in the practical benefits of adopting LangGraph for startups and how it can provide a competitive edge.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Samuel Patel\n",
      "Affiliation: AI Research Institute\n",
      "Role: Lead Researcher\n",
      "Description: Dr. Patel specializes in AI frameworks and their applications in various industries. His focus is on the technical advantages of LangGraph, including its scalability and integration capabilities. He aims to provide insights into how adopting LangGraph can lead to improved performance and efficiency in AI systems.\n",
      "--------------------------------------------------\n",
      "Name: Maria Gomez\n",
      "Affiliation: Tech Consulting Firm\n",
      "Role: Senior Analyst\n",
      "Description: Maria works as a senior analyst at a tech consulting firm, where she advises clients on AI adoption strategies. She is interested in the business implications of adopting LangGraph, including cost-effectiveness, ease of implementation, and the potential for enhancing client solutions through advanced agent frameworks.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0af41f54-88d9-4597-98b0-444c08322095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f09bc03-042d-6af9-8004-ce79d93d6120'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm we are happy\n",
    "graph.update_state(thread, {\"human_analyst_feedback\": \n",
    "                            None}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37123ca7-c20b-43c1-9a71-39ba344e7ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Node--\n",
      "conduct_interview\n",
      "--Node--\n",
      "conduct_interview\n",
      "--Node--\n",
      "conduct_interview\n",
      "--Node--\n",
      "write_conclusion\n",
      "--Node--\n",
      "write_introduction\n",
      "--Node--\n",
      "write_report\n",
      "--Node--\n",
      "finalize_report\n"
     ]
    }
   ],
   "source": [
    "# Continue\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8f66ad8-80fd-4eb2-96b6-6ae9dffd060c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Harnessing the Power of LangGraph: A Framework for Innovation\n",
       "\n",
       "## Introduction\n",
       "\n",
       "In the fast-paced world of generative AI, startups are in constant pursuit of tools that drive productivity and innovation. LangGraph, an open-source AI agent framework developed by LangChain, stands out by simplifying the creation and management of complex AI workflows. Its graph-based architecture enables developers to model intricate relationships, making it ideal for diverse applications, from robotics to customer service. With modular design for scalability, built-in state management for reliability, and advanced natural language processing capabilities, LangGraph empowers startups to enhance user experiences and streamline operations. As the generative AI landscape evolves, adopting LangGraph is essential for staying competitive and fostering innovation.\n",
       "\n",
       "---\n",
       "\n",
       "\n",
       "The adoption of LangGraph as an agent framework presents numerous benefits for startups and enterprises alike, particularly in the rapidly evolving field of artificial intelligence (AI). Developed by LangChain, LangGraph is an open-source AI agent framework that streamlines the creation, deployment, and management of complex generative AI workflows. Its graph-based architecture allows for intricate modeling of relationships within AI systems, making it particularly advantageous for applications in robotics, autonomous vehicles, and customer service automation [1][2].\n",
       "\n",
       "One of the standout features of LangGraph is its modular design, which supports scalability and adaptability. This flexibility enables organizations to expand workflows, introduce new agents, or adjust logic without starting from scratch, significantly reducing development time and costs [5]. Such capabilities are essential for lean teams that require rapid iteration cycles and the ability to prototype quickly. For example, Norwegian Cruise Line has successfully leveraged LangGraph to enhance guest-facing AI solutions, showcasing its practical benefits in real-world applications [1][3].\n",
       "\n",
       "LangGraph excels in managing complex workflows that require extensive context management and multi-agent interactions. Its ability to optimize response time and accuracy through advanced natural language processing capabilities enhances user experience and improves decision-making by allowing AI agents to make informed choices based on structured data [2][3]. Furthermore, the framework's integration capabilities with existing ecosystems provide a competitive edge, allowing for the incorporation of various tools and technologies without disrupting workflows [1][4].\n",
       "\n",
       "Despite its advantages, the rapid evolution of LangGraph presents challenges, such as documentation that often lags behind its development, leading to a steep learning curve for new users [1]. Additionally, initial latency in data synchronization and the need for conversation memory to maintain context across interactions have been noted as potential hurdles. However, innovative solutions like incremental sync and feedback loops have been implemented to address these issues, improving response quality over time [5][6].\n",
       "\n",
       "In summary, LangGraph not only meets the technical demands of modern AI applications but also positions itself as a forward-thinking framework that can evolve alongside industry needs. Its unique features and capabilities make it a valuable asset for businesses aiming to enhance their AI-driven processes, providing a structured framework that enhances decision-making and operational efficiency. As the generative AI landscape continues to evolve, adopting frameworks like LangGraph will be essential for organizations looking to maintain a competitive edge.\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In conclusion, adopting LangGraph as an agent framework offers significant advantages for startups and enterprises alike in the generative AI landscape. Its modular design and graph-based architecture empower organizations to create scalable, adaptable workflows that enhance productivity and innovation. By facilitating complex reasoning and multi-agent interactions, LangGraph not only improves operational efficiency but also enriches user experiences through advanced natural language processing. Despite some challenges, such as documentation gaps and initial latency, the framework's potential to streamline AI development and deliver tailored solutions makes it a strategic asset for businesses aiming to stay competitive in an evolving market. Embracing LangGraph is not just a technical choice; it is a forward-thinking investment in the future of AI.\n",
       "\n",
       "## Sources\n",
       "[1] https://www.ibm.com/think/topics/langgraph  \n",
       "[2] https://medium.com/@ronivaldo/stop-using-langgraph-and-crewai-build-superior-ai-agents-with-pure-python-3baec44eb451  \n",
       "[3] https://www.langchain.com/langgraph  \n",
       "[4] https://oyelabs.com/langgraph-vs-crewai-vs-openai-swarm-ai-agent-framework/  \n",
       "[5] https://www.ema.co/additional-blogs/addition-blogs/building-ai-agents-langgraph  \n",
       "[6] https://www.rapidinnovation.io/post/ai-agents-in-langgraph  \n",
       "[7] https://latenode.com/blog/langgraph-vs-autogen-vs-crewai-complete-ai-agent-framework-comparison-architecture-analysis-2025  \n",
       "[8] https://designveloper.com/blog/what-is-langgraph/  \n",
       "[9] https://adasci.org/a-practical-guide-to-building-ai-agents-with-langgraph/  \n",
       "[10] https://blog.langchain.com/building-langgraph/  \n",
       "[11] https://www.royalcyber.com/blogs/ai-ml/enterprise-ai-chatbot-implementation/  \n",
       "[12] https://gyliu513.medium.com/ai-agents-with-langgraph-agentic-workflow-pattern-36d867dc7b68"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "final_state = graph.get_state(thread)\n",
    "report = final_state.values.get('final_report')\n",
    "Markdown(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf8edd-fb42-496c-9bdb-3f5d7b4d79d3",
   "metadata": {},
   "source": [
    "We can look at the trace:\n",
    "\n",
    "https://smith.langchain.com/public/2933a7bb-bcef-4d2d-9b85-cc735b22ca0c/r\n",
    "\n",
    "### We can see multiple \"conduct_interview\" nodes in the trace. This is because we are using the Send() API to run each interview in parallel. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
